{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_training_file = \"training_output_binary_filename.csv\"\n",
    "default_model_choice = \"NB\"\n",
    "\n",
    "#NB specific setting\n",
    "default_NB_features = \"n-gram\" #\"bag of words\" \"tf-idf\" \"n-gram\"\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "training_file = default_training_file\n",
    "model_choice = default_model_choice\n",
    "NB_features = default_NB_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train/test split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split dataset evenly based on labels\n",
    "def split_test_train(total, stratify_col):\n",
    "    transition_rows = total[total[stratify_col] != 0]\n",
    "    non_transition_rows = total[total[stratify_col] == 0]\n",
    "    \n",
    "    # first split transitions into training/testing\n",
    "    X_train1, X_test1, y_train1, y_test1 = train_test_split(transition_rows, \n",
    "                                                    transition_rows['transition_value'], \n",
    "                                                    test_size=0.30, random_state=42)\n",
    "    \n",
    "    # assert there are only transition labels in this dataframe\n",
    "    assert len(X_train1[X_train1['transition_value'] == 0]) == 0\n",
    "    assert len(X_test1[X_test1['transition_value'] == 0]) == 0\n",
    "    \n",
    "    train_len = len(X_train1) # number of non-transitions to add to training set\n",
    "    test_len = len(X_test1) # number of non-transitions to add to testing set\n",
    "    \n",
    "    \n",
    "    # next split non-transitions into training/testing\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(non_transition_rows, \n",
    "                                                    non_transition_rows['transition_value'], \n",
    "                                                    test_size=0.30, random_state=42)\n",
    "    \n",
    "    # pick train_len random rows from non-transition training set\n",
    "    X_train2 = X_train2.sample(n = train_len, axis=0)\n",
    "    \n",
    "    # pick test_len random rows from non_transitions testing set\n",
    "    X_test2 = X_test2.sample(n = test_len, axis=0)\n",
    "    \n",
    "    # assert there are no transition utterances in non-transition training and testing set\n",
    "    assert len(X_train2[X_train2['transition_value'] != 0]) == 0\n",
    "    assert len(X_test2[X_test2['transition_value'] != 0]) == 0\n",
    "    \n",
    "    # final result, concat the dataframe\n",
    "    X_train_final = pd.concat([X_train1, X_train2])\n",
    "    X_test_final = pd.concat([X_test1, X_test2])\n",
    "    \n",
    "    print (X_train_final.head())\n",
    "    \n",
    "    return X_train_final['text'], X_test_final['text'], X_train_final['transition_value'], X_test_final['transition_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assert training/testing split is balanced\n",
    "def verify_train_test_split(train, x_train, y_train, x_test, y_test):\n",
    "    transition_rows = train[train[\"transition_value\"] != 0]\n",
    "    assert len(x_train) == len(y_train)\n",
    "    assert len(x_test) == len(y_test)\n",
    "    assert len(x_train) == int(len(transition_rows) * 0.7) * 2\n",
    "    assert len(x_test) == (len(transition_rows) * 2) - (int(len(transition_rows) * 0.7) * 2)\n",
    "    assert len(y_train[y_train == 0]) == len(y_train[y_train != 0])\n",
    "    assert len(y_test[y_test == 0]) == len(y_test[y_test != 0])\n",
    "    print (\"{0}% of utterances are transitions\".format((sum(y_train) / len(x_train)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define naive bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract bag of words features from text for a model\n",
    "def bag_of_words_features(x_train, x_test):\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit(np.hstack((x_train)))\n",
    "    X_train_counts = count_vect.transform(x_train)\n",
    "    X_test_counts = count_vect.transform(x_test)\n",
    "    \n",
    "    assert X_train_counts.shape[1] == X_test_counts.shape[1]\n",
    "    \n",
    "    return X_train_counts, X_test_counts, count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_tfidf(x_train, x_test):\n",
    "    X_train_counts, X_test_counts, count_vect = bag_of_words_features(x_train, x_test)\n",
    "    \n",
    "    transformer = TfidfTransformer(smooth_idf=True)\n",
    "    Xtrain_tfidf = transformer.fit_transform(X_train_counts)\n",
    "    Xtest_tfidf = transformer.fit_transform(X_test_counts)\n",
    "    \n",
    "    assert Xtrain_tfidf.shape[1] == Xtest_tfidf.shape[1]\n",
    "    \n",
    "    return Xtrain_tfidf, Xtest_tfidf, count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_ngram(start, stop, x_train, x_test):\n",
    "    ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(start, stop))\n",
    "    counts = ngram_vectorizer.fit(np.hstack((x_train)))\n",
    "    \n",
    "    #print (\"Number of transformed features {0}\\n\"\n",
    "    # .format(len(ngram_vectorizer.get_feature_names())))\n",
    "    \n",
    "    #print (\"First 10 features\\n{0}\"\n",
    "    # .format('\\n'.join(ngram_vectorizer.get_feature_names()[-10:])))\n",
    "    \n",
    "    X_train_counts = counts.transform(x_train)\n",
    "    X_test_counts = counts.transform(x_test)\n",
    "    \n",
    "    assert X_train_counts.shape[1] == X_test_counts.shape[1]\n",
    "    \n",
    "    return X_train_counts, X_test_counts, ngram_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(x_train, x_test):\n",
    "    if (NB_features == \"bag of words\"):\n",
    "        return bag_of_words_features(x_train, x_test)\n",
    "    \n",
    "    elif (NB_features == \"tf-idf\"):\n",
    "        return transform_tfidf(x_train, x_test)\n",
    "    \n",
    "    elif (NB_features == \"n-gram\"):\n",
    "        return transform_ngram(1, 6, x_train, x_test)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Feature set {0} it not supported\"\n",
    "         .format(NB_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output accuracy for a naive bayes model\n",
    "# return the trained model\n",
    "def create_naive_bayes_model(x_train, x_test, y_train, y_test):\n",
    "    X_train_counts, X_test_counts, count_vect = features(x_train, x_test)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_counts, y_train)\n",
    "    \n",
    "    assert X_test_counts.shape[0] == y_test.shape[0]\n",
    "    \n",
    "    acc = clf.score(X_test_counts, y_test, sample_weight=None)\n",
    "    print(\"Model accuracy {0}\".format(acc))\n",
    "    \n",
    "    return clf, count_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neutral network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_neural_network_model(x_train, x_test, y_train, y_test):\n",
    "    #tokenize and pad word length\n",
    "    tokenizer = Tokenizer(num_words=40000)\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "    padded = pad_sequences(sequences, maxlen = 44)\n",
    "    pred = to_categorical(y_train)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(40000, 150, input_length=44))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.5))\n",
    "    model.add(Dense(2, activation='sigmoid')) #fully connected layer\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    filepath=\"weights.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.fit(padded, pred,validation_split=0.3, epochs = 50, callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(training_file, sep=\"~\")[['video_id', 'transition_value', 'text']]\n",
    "train['transition_value'][train[\"transition_value\"]==2] = 0\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_test_train(train[['text', 'transition_value']], \"transition_value\")\n",
    "verify_train_test_split(train, x_train, y_train, x_test, y_test)\n",
    "\n",
    "x_train.head()\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (model_choice == \"NN\"):\n",
    "    raise Exception(\"Neural network not supported yet.\")\n",
    "    create_neural_network_model(x_train, x_test, y_train, y_test)\n",
    "elif (model_choice == \"NB\"):\n",
    "    model, count_vect = create_naive_bayes_model(x_train, x_test, y_train, y_test)\n",
    "    pickle.dump(model, open(\"nb_model.p\", \"wb\"))\n",
    "    pickle.dump(count_vect, open(\"nb_count_vect.p\", \"wb\"))\n",
    "else:\n",
    "    raise Exception(\"No model provided.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
