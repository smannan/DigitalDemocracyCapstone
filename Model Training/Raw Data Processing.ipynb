{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the raw .srt file that will be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_original_raw_filename = \"../data/original/raw.txt\"\n",
    "default_cleaned_raw_filename = \"../data/cleaned/cleaned_raw.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_raw_filename = default_original_raw_filename\n",
    "cleaned_raw_filename = default_cleaned_raw_filename\n",
    "cleaned_raw_bill_id_replaced_filename = cleaned_raw_filename + \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the raw transcript specified above, and verify it was read in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = pd.read_table(original_raw_filename, sep='~~~~~', engine='python')\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Define parsing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse a string 00:00:00.470 to hours, minutes, seconds\n",
    "# return time in seconds\n",
    "def parse_time(time):\n",
    "    time = time.split(\":\")\n",
    "    hours = int(time[0])\n",
    "    minutes = int(time[1])\n",
    "    seconds = int(float(time[2])) \n",
    "    \n",
    "    return (hours*360)+(minutes*60)+seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_raw_data(raw):\n",
    "    r = raw['raw_transcript']\n",
    "    ids = raw['video_id']\n",
    "    res = {'start':[], 'end':[], 'text':[], 'video_id': []}\n",
    "    for transcript, vid in zip(r, ids):\n",
    "        soup = BeautifulSoup(transcript, \"lxml\")\n",
    "        letters = soup.find_all(\"p\")\n",
    "\n",
    "        for p in letters[1:]:\n",
    "            res['start'].append(parse_time(p.get('begin')))\n",
    "            res['end'].append(parse_time(p.get('end')))\n",
    "            res['text'].append(p.contents[0])\n",
    "            res['video_id'].append(vid)\n",
    "\n",
    "    tidy = pd.DataFrame(res, columns=['start', 'end', 'text', 'video_id'])\n",
    "    \n",
    "    return (tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define text formatting and bill replacement logic.  This converts all utterances to entirely lowercase, and replaces the following instances of words in an utterance with the tag BILL_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bill_id_pattern_1_1 = \"ab[0-9]+\"\n",
    "bill_id_pattern_1_2 = \"sb[0-9]+\"\n",
    "bill_id_pattern_1_3 = \"aca[0-9]+\"\n",
    "bill_id_pattern_1_4 = \"acr[0-9]+\"\n",
    "bill_id_pattern_1_5 = \"ajr[0-9]+\"\n",
    "bill_id_pattern_1_6 = \"ar[0-9]+\"\n",
    "bill_id_pattern_1_7 = \"hr[0-9]+\"\n",
    "bill_id_pattern_1_8 = \"sca[0-9]+\"\n",
    "bill_id_pattern_1_9 = \"scr[0-9]+\"\n",
    "bill_id_pattern_1_10 = \"sjr[0-9]+\"\n",
    "\n",
    "bill_id_pattern_2_1 = [\"ab\", \"[0-9]+\"]\n",
    "bill_id_pattern_2_2 = [\"sb\", \"[0-9]+\"]\n",
    "bill_id_pattern_2_3 = [\"aca\", \"[0-9]+\"]\n",
    "bill_id_pattern_2_4 = [\"acr\", \"[0-9]+\"]\n",
    "bill_id_pattern_2_5 = [\"ajr\", \"[0-9]+\"]\n",
    "bill_id_pattern_2_6 = [\"ar\", \"[0-9]+\"]\n",
    "bill_id_pattern_2_7 = [\"hr\", \"[0-9]+\"]\n",
    "bill_id_pattern_2_8 = [\"sca\", \"[0-9]+\"]\n",
    "bill_id_pattern_2_9 = [\"scr\", \"[0-9]+\"]\n",
    "bill_id_pattern_2_10 = [\"sjr\", \"[0-9]+\"]\n",
    "\n",
    "bill_id_pattern_3_1 = [\"assembly\", \"bill\", \"[0-9]+\"]\n",
    "bill_id_pattern_3_2 = [\"senate\", \"bill\", \"[0-9]+\"]\n",
    "\n",
    "bill_id_pattern_4_1 = [\"assembly\", \"bill\", \"number\", \"[0-9]+\"]\n",
    "bill_id_pattern_4_2 = [\"senate\", \"bill\", \"number\", \"[0-9]+\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def re_match_lists_helper(pattern_list, word_list):\n",
    "    for p in range(len(pattern_list)):\n",
    "        if not (re.match(pattern_list[p], word_list[p])):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def re_match_lists(pattern_list_list, word_list):\n",
    "    for pl in range(len(pattern_list_list)):\n",
    "        if (re_match_lists_helper(pattern_list_list[pl], word_list)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def matches_any_4_word_pattern(word1, word2, word3, word4):\n",
    "    pattern_list_list = [bill_id_pattern_4_1, bill_id_pattern_4_2]\n",
    "    word_list = [word1, word2, word3, word4]\n",
    "    \n",
    "    return re_match_lists(pattern_list_list, word_list)\n",
    "\n",
    "def matches_any_3_word_pattern(word1, word2, word3):\n",
    "    pattern_list_list = [bill_id_pattern_3_1, bill_id_pattern_3_2]\n",
    "    word_list = [word1, word2, word3]\n",
    "    \n",
    "    return re_match_lists(pattern_list_list, word_list)\n",
    "    \n",
    "def matches_any_2_word_pattern(word1, word2):\n",
    "    pattern_list_list = [bill_id_pattern_2_1, bill_id_pattern_2_2,\n",
    "                         bill_id_pattern_2_3, bill_id_pattern_2_4,\n",
    "                         bill_id_pattern_2_5, bill_id_pattern_2_6,\n",
    "                         bill_id_pattern_2_7, bill_id_pattern_2_8,\n",
    "                         bill_id_pattern_2_9, bill_id_pattern_2_10]\n",
    "    word_list = [word1, word2]\n",
    "    \n",
    "    return re_match_lists(pattern_list_list, word_list)\n",
    "\n",
    "def matches_any_1_word_pattern(word):\n",
    "    return (re.match(bill_id_pattern_1_1, word) or\n",
    "            re.match(bill_id_pattern_1_2, word) or\n",
    "            re.match(bill_id_pattern_1_3, word) or\n",
    "            re.match(bill_id_pattern_1_4, word) or\n",
    "            re.match(bill_id_pattern_1_5, word) or\n",
    "            re.match(bill_id_pattern_1_6, word) or\n",
    "            re.match(bill_id_pattern_1_7, word) or\n",
    "            re.match(bill_id_pattern_1_8, word) or\n",
    "            re.match(bill_id_pattern_1_9, word) or\n",
    "            re.match(bill_id_pattern_1_10, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_words_over(words, word_ix, shift_amount):\n",
    "    words_length = len(words)\n",
    "    \n",
    "    for i in range(word_ix, words_length - shift_amount):\n",
    "        words[i] = words[i+shift_amount]\n",
    "    while(len(words) > (words_length-shift_amount)):\n",
    "        del words[-1]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_bill_ids_in_utterance(utterance, last_bill_number, t1, t2, t3, t4):\n",
    "    words = utterance.lower().split()\n",
    "    utterance_length = len(words)\n",
    "    word_ix = 0\n",
    "    bill_id_replaced = False\n",
    "    while(word_ix < utterance_length):\n",
    "        if (word_ix < (utterance_length-3) and\n",
    "            matches_any_4_word_pattern(words[word_ix],\n",
    "                                         words[word_ix+1],\n",
    "                                         words[word_ix+2],\n",
    "                                         words[word_ix+3])):\n",
    "            last_bill_number = words[word_ix+3]\n",
    "            words[word_ix] = \"<BILL_ID>\"\n",
    "            words = shift_words_over(words, word_ix+1, 3)\n",
    "            utterance_length -= 3\n",
    "            bill_id_replaced = True\n",
    "            t4 += 1\n",
    "        elif (word_ix < (utterance_length-2) and\n",
    "              matches_any_3_word_pattern(words[word_ix],\n",
    "                                         words[word_ix+1],\n",
    "                                         words[word_ix+2])):\n",
    "            last_bill_number = words[word_ix+2]\n",
    "            words[word_ix] = \"<BILL_ID>\"\n",
    "            words = shift_words_over(words, word_ix+1, 2)\n",
    "            utterance_length -= 2\n",
    "            bill_id_replaced = True\n",
    "            t3 += 1\n",
    "        elif (word_ix < (utterance_length-1) and\n",
    "            matches_any_2_word_pattern(words[word_ix],\n",
    "                                         words[word_ix+1])):\n",
    "            last_bill_number = words[word_ix+1]\n",
    "            words[word_ix] = \"<BILL_ID>\"\n",
    "            words = shift_words_over(words, word_ix+1, 1)\n",
    "            utterance_length -= 1\n",
    "            bill_id_replaced = True\n",
    "            t2 += 1\n",
    "        elif (matches_any_1_word_pattern(words[word_ix])):\n",
    "            last_bill_number = words[word_ix].split(\"[a-z]+\")[-1]\n",
    "            words[word_ix] = \"<BILL_ID>\"\n",
    "            bill_id_replaced = True\n",
    "            t1 += 1\n",
    "\n",
    "        word_ix += 1\n",
    "            \n",
    "    return (\" \".join(words), last_bill_number, bill_id_replaced, t1, t2, t3, t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_bill_ids(old, new):\n",
    "    t1 = 0  #keeps track of how many bill id replacements there were\n",
    "    t2 = 0\n",
    "    t3 = 0\n",
    "    t4 = 0\n",
    "    \n",
    "    last_bill_number = \"\"\n",
    "    last_bill_number_line = 0\n",
    "    transition_window_list = []\n",
    "    line_number = 0\n",
    "    for line in old:\n",
    "        line_splits = line.lower().rstrip(\"\\n\").split(\"~\")\n",
    "        \n",
    "        (new_text, current_bill_number, bill_id_replaced, t1, t2, t3, t4) = replace_bill_ids_in_utterance(line_splits[2], last_bill_number, t1, t2, t3, t4)\n",
    "        \n",
    "        if (bill_id_replaced):\n",
    "            if (current_bill_number != last_bill_number):\n",
    "                transition_window_list.append((last_bill_number_line, line_number))\n",
    "                last_bill_number = current_bill_number\n",
    "                last_bill_number_line = line_number\n",
    "            elif (current_bill_number == last_bill_number):\n",
    "                last_bill_number_line = line_number\n",
    "        \n",
    "        new.write(line_splits[0] + \"~\" + line_splits[1] + \"~\" + new_text + \"~\" + line_splits[3] + \"\\n\")\n",
    "        line_number += 1\n",
    "        \n",
    "    #print(\"Length of Bill Patterns Replaced\\n1: \" + str(t1) + \"\\n2: \" + str(t2) + \"\\n3: \" + str(t3) + \"\\n4: \" + str(t4))\n",
    "    return transition_window_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define add context function (prefix and postfix words in surrounding utterances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adds the prefix POST to all utterances n after\n",
    "# adds the prefix PRE to all utterances n before\n",
    "# a transition phrase\n",
    "def add_context_naive_bayes(n):\n",
    "    n_range = pd.read_csv(cleaned_raw_bill_id_replaced_filename, sep=\"~\")\n",
    "    \n",
    "    transition_text = n_range['text']\n",
    "    new_transition_text = []\n",
    "\n",
    "    length = len(n_range)\n",
    "    \n",
    "    for i in range(length):\n",
    "        # get the phrases in the window\n",
    "        text = ''\n",
    "        for x in range(-n, n+1):\n",
    "            # window is within range of the dataframe\n",
    "            if (i + x >= 0 and i + x < length):\n",
    "                if (x > 0):\n",
    "                    text += ' '.join([\"POST-\" + element for element in transition_text[i+x].split()])\n",
    "                if (x < 0):\n",
    "                    text += ' '.join([\"PRE-\" + element for element in transition_text[i+x].split()])\n",
    "                else:\n",
    "                    text += \" {0} \".format(str(transition_text[i+x]))\n",
    "                    \n",
    "        new_transition_text.append(text)\n",
    "    \n",
    "    print (\"Number of new phrases {0}\".format(len(new_transition_text)))\n",
    "    \n",
    "    n_range.drop(['text'], axis=1, inplace=True)\n",
    "    n_range['text'] = new_transition_text\n",
    "    \n",
    "    n_range.to_csv(cleaned_raw_bill_id_replaced_filename, sep=\"~\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates three columns:\n",
    "# utterance text\n",
    "# pre-utterance text\n",
    "# post-utterance text\n",
    "def add_context_neural_network(n, input_file):\n",
    "    n_range = pd.read_csv(cleaned_raw_bill_id_replaced_filename, sep=\"~\")\n",
    "    \n",
    "    transition_text = n_range['text']\n",
    "    new_transition_text = []\n",
    "    pre_transition_text = []\n",
    "    post_transition_text = []\n",
    "\n",
    "    length = len(n_range)\n",
    "    \n",
    "    for i in range(length):\n",
    "        # get the phrases in the window\n",
    "        target_text = ''\n",
    "        pre = ''\n",
    "        post = ''\n",
    "        \n",
    "        for x in range(-n, n+1):\n",
    "            # window is within range of the dataframe\n",
    "            if (i + x >= 0 and i + x < length):\n",
    "                if (x > 0):\n",
    "                    post += ' '.join([\"POST-\" + element for element in transition_text[i+x].split()])\n",
    "                if (x < 0):\n",
    "                    pre += ' '.join([\"PRE-\" + element for element in transition_text[i+x].split()])\n",
    "                else:\n",
    "                    target_text += \" {0} \".format(str(transition_text[i+x]))\n",
    "                    \n",
    "        new_transition_text.append(target_text)\n",
    "        pre_transition_text.append(pre)\n",
    "        post_transition_text.append(post)\n",
    "    \n",
    "    print (\"Number of new phrases {0}\".format(len(new_transition_text)))\n",
    "    \n",
    "    n_range.drop(['text'], axis=1, inplace=True)\n",
    "    n_range['text'] = new_transition_text\n",
    "    n_range['post_text'] = post_transition_text\n",
    "    n_range['pre_text'] = pre_transition_text\n",
    "    \n",
    "    #n_range.to_csv(output_file, sep=\"~\", index=False)\n",
    "    return n_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually parse the raw transcript and replace bill ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"...Parsing raw transcript...\")\n",
    "\n",
    "cleaned_raw = parse_raw_data(raw)\n",
    "cleaned_raw = cleaned_raw.sort_values([\"video_id\", \"start\"])\n",
    "\n",
    "cleaned_raw.to_csv(cleaned_raw_filename, sep=\"~\", index=False)\n",
    "\n",
    "# remove transcripts for testing\n",
    "# original cleaned_raw.to_csv(cleaned_raw_filename, sep=\"~\", index=False)\n",
    "\n",
    "video_ids = np.unique(cleaned_raw['video_id'])[1:11:3]\n",
    "print ('Number of video ids {0}'.format(video_ids))\n",
    "\n",
    "video_ids = np.unique(cleaned_raw['video_id'])[1:11:3]\n",
    "cleaned_raw_final = cleaned_raw[~cleaned_raw['video_id'].isin(video_ids)]\n",
    "\n",
    "test = cleaned_raw[cleaned_raw['video_id'].isin(video_ids)]\n",
    "\n",
    "cleaned_raw_final.to_csv(cleaned_raw_filename, sep=\"~\", index=False)\n",
    "test.to_csv(\"../data/cleaned/transcript_witheld.csv\", sep='~', index=False)\n",
    "\n",
    "print(\"...Raw transcript parsed, beginning text formatting and bill replacement...\")\n",
    "\n",
    "transition_window_list = [] #not currently used, but is available for use\n",
    "\n",
    "with open(cleaned_raw_filename, 'r') as old:\n",
    "    with open(cleaned_raw_bill_id_replaced_filename, 'w') as new:\n",
    "        # consume/write headings\n",
    "        h = old.readline()\n",
    "        new.write(h)\n",
    "            \n",
    "        #actually iterate through the file\n",
    "        transition_window_list = replace_bill_ids(old, new)\n",
    "\n",
    "print(\"...Text formatted and bills replaced, adding context...\")\n",
    "\n",
    "add_context_naive_bayes(5)\n",
    "\n",
    "print(\"...Context added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n_range = add_context_neural_network(5,cleaned_raw_bill_id_replaced_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n_range.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n_range.to_csv('/Users/soniamannan/Documents/DATA401/capstone/DigitalDemocracyCapstone/data/cleaned/neural_network_context.csv',index=False,sep=\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
